{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "dfa5d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbd797",
   "metadata": {},
   "source": [
    "### Task 1 - Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "74bb904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering Task 1\n",
    "def Engineering_Task1(data):\n",
    "    # Iterate through each column\n",
    "    for col in data.columns:\n",
    "        # Check if the column contains missing values\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            # If the column is categorical, impute with the most frequent value\n",
    "            if data[col].dtype == 'object':\n",
    "                data[col].fillna(data[col].value_counts().index[0], inplace=True)\n",
    "\n",
    "        # If the column is numerical, impute with the mean value\n",
    "            else:\n",
    "                data[col].fillna(data[col].mean(), inplace=True)\n",
    "\n",
    "# Save the imputed dataset\n",
    "    data.to_csv('imputed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "5e3c033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "              names=[\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"Salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "862447ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  Salary  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "feb2547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert the target variable to numerical values using label encoding\n",
    "le = LabelEncoder()\n",
    "\n",
    "Engineering_Task1(data)\n",
    "X = data.drop(['Salary'], axis=1).values\n",
    "y = data['Salary'].values\n",
    "# print(X)\n",
    "# print(y)\n",
    "# y[y==' <=50K']=0\n",
    "# y[y==' >50K']=1\n",
    "# print([y==0].sum)\n",
    "# assume X and y are the feature matrix and target vector, respectively\n",
    "\n",
    "# shuffle the indices of the data\n",
    "indices = list(range(len(X)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# calculate the split point\n",
    "split_idx = int(0.67 * len(X))\n",
    "\n",
    "\n",
    "# split the data into training and testing sets\n",
    "train_X = X[indices[:split_idx]]\n",
    "train_y = y[indices[:split_idx]]\n",
    "test_X = X[indices[split_idx:]]\n",
    "test_y = y[indices[split_idx:]]\n",
    "\n",
    "train_y = le.fit_transform(train_y)\n",
    "test_y = le.transform(test_y)\n",
    "# print(train_X[:,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf7ccfd",
   "metadata": {},
   "source": [
    "### Task 2 - Naive Bayes Classifier Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26436868",
   "metadata": {},
   "source": [
    "### 1. Calculation of the prior probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "054cb5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior_probabilities(labels):\n",
    "    num_samples = len(labels)\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    prior_probs = {}\n",
    "    \n",
    "    for label, label_count in zip(unique_labels, label_counts):\n",
    "        prior_probs[label] = label_count / num_samples\n",
    "    \n",
    "    return prior_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "f70a441d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.7613568645427459, 1: 0.23864313545725419}\n"
     ]
    }
   ],
   "source": [
    "prior_prob=calculate_prior_probabilities(train_y)\n",
    "print(prior_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a73e76",
   "metadata": {},
   "source": [
    "### 2.  Calculation of the conditional probability of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "ea9693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conditional_probabilities(features, target):\n",
    "    num_features = features.shape[1]\n",
    "#     print(features.names)\n",
    "#     print(num_features)\n",
    "    unique_targets = np.unique(target)\n",
    "#     print(unique_targets)\n",
    "    conditional_probs = {}\n",
    "\n",
    "    for label in unique_targets:\n",
    "        label_features = features[target == label]\n",
    "        conditional_probs[label] = {}\n",
    "\n",
    "        for i in range(num_features):\n",
    "            feature_values, feature_counts = np.unique(label_features[:, i], return_counts=True)\n",
    "#             print(feature_values,feature_counts)\n",
    "            total_count = np.sum(feature_counts)\n",
    "#             print(total_count)\n",
    "            prob_dict = {value: count/total_count for value, count in zip(feature_values, feature_counts)}\n",
    "            conditional_probs[label][i] = prob_dict\n",
    "\n",
    "    return conditional_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "8585e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_prob=calculate_conditional_probabilities(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e4d25",
   "metadata": {},
   "source": [
    "### 3. Predict the class of a given instance using the Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "a534f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(instance, prior_probs, conditional_probs):\n",
    "    unique_labels = list(prior_probs.keys())\n",
    "    num_features = len(instance)\n",
    "\n",
    "    class_probs = {}\n",
    "    for label in unique_labels:\n",
    "        class_probs[label] = prior_probs[label]\n",
    "\n",
    "        for i in range(num_features):\n",
    "            feature_value = instance[i]\n",
    "            if feature_value in conditional_probs[label][i]:\n",
    "                class_probs[label] *= conditional_probs[label][i][feature_value]\n",
    "            else:\n",
    "                class_probs[label] *=0\n",
    "#     print(class_probs)\n",
    "    return max(class_probs, key=class_probs.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "5e4154de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for i in range(len(test_X)):\n",
    "    predictions.append(predict(test_X[i],prior_prob,cond_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a5c1e",
   "metadata": {},
   "source": [
    "### 4. Function to calculate the accuracy of your Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "ef5e38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, actual_labels):\n",
    "    num_correct = sum(predictions == actual_labels)\n",
    "    accuracy = num_correct / len(actual_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "048c8a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764749674297413"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(predictions, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423d489",
   "metadata": {},
   "source": [
    "### Task 3 - Evaluation and Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac630cd9",
   "metadata": {},
   "source": [
    "### 1. Accuracy, Precision, Recall, and F1-score of Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "e6d2780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_performance(predictions, actual_labels):\n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    precision = precision_score(actual_labels, predictions, average='weighted')\n",
    "    recall = recall_score(actual_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(actual_labels, predictions, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c3ce8c65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.764749674297413, 0.7355776253273522, 0.764749674297413, 0.7384193433160445)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_performance(predictions, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7362a99",
   "metadata": {},
   "source": [
    "###  2. Smoothing Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8fb55945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_smoothing(instance, prior_probs, conditional_probs):\n",
    "    unique_labels = list(prior_probs.keys())\n",
    "    num_features = len(instance)\n",
    "\n",
    "    class_probs = {}\n",
    "    for label in unique_labels:\n",
    "        class_probs[label] = prior_probs[label]\n",
    "\n",
    "        for i in range(num_features):\n",
    "            feature_value = instance[i]\n",
    "            if feature_value in conditional_probs[label][i]:\n",
    "                class_probs[label] *= conditional_probs[label][i][feature_value]\n",
    "            else:\n",
    "                class_probs[label] *=1e-3\n",
    "#     print(class_probs)\n",
    "    return max(class_probs, key=class_probs.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "c303cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_smoothing=[]\n",
    "for i in range(len(test_X)):\n",
    "    predictions_smoothing.append(predict_smoothing(test_X[i],prior_prob,cond_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "192f6430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7963893541782989,\n",
       " 0.8331712795819932,\n",
       " 0.7963893541782989,\n",
       " 0.8065158939442165)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_performance(predictions_smoothing, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d72fc",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes VS  Logistic Regression VS K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "4c91e5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nobel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier:\n",
      "Accuracy: 0.87\n",
      "Precision: 0.79\n",
      "Recall: 0.62\n",
      "F1-score: 0.70\n",
      "\n",
      "k-Nearest Neighbors classifier:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.66\n",
      "Recall: 0.54\n",
      "F1-score: 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# create OneHotEncoder object\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# fit encoder on categorical features\n",
    "enc.fit(train_X)\n",
    "\n",
    "# transform categorical features to one-hot encoded features\n",
    "X_train_enc = enc.transform(train_X).toarray()\n",
    "X_test_enc = enc.transform(test_X).toarray()\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "X_train, y_train, X_test, y_test = X_train_enc,train_y,X_test_enc,test_y\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "\n",
    "# Train and test a logistic regression classifier\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "prec_lr = precision_score(y_test, y_pred_lr)\n",
    "rec_lr = recall_score(y_test, y_pred_lr)\n",
    "f1_lr = f1_score(y_test, y_pred_lr)\n",
    "\n",
    "# Train and test a k-nearest neighbors classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "prec_knn = precision_score(y_test, y_pred_knn)\n",
    "rec_knn = recall_score(y_test, y_pred_knn)\n",
    "f1_knn = f1_score(y_test, y_pred_knn)\n",
    "\n",
    "# Print the results\n",
    "# print(\"Naive Bayes classifier:\")\n",
    "# print(\"Accuracy: {:.2f}\".format(acc_nb))\n",
    "# print(\"Precision: {:.2f}\".format(prec_nb))\n",
    "# print(\"Recall: {:.2f}\".format(rec_nb))\n",
    "# print(\"F1-score: {:.2f}\".format(f1_nb))\n",
    "# print()\n",
    "print(\"Logistic regression classifier:\")\n",
    "print(\"Accuracy: {:.2f}\".format(acc_lr))\n",
    "print(\"Precision: {:.2f}\".format(prec_lr))\n",
    "print(\"Recall: {:.2f}\".format(rec_lr))\n",
    "print(\"F1-score: {:.2f}\".format(f1_lr))\n",
    "print()\n",
    "print(\"k-Nearest Neighbors classifier:\")\n",
    "print(\"Accuracy: {:.2f}\".format(acc_knn))\n",
    "print(\"Precision: {:.2f}\".format(prec_knn))\n",
    "print(\"Recall: {:.2f}\".format(rec_knn))\n",
    "print(\"F1-score: {:.2f}\".format(f1_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d2a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
